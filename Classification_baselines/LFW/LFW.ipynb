{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled Faces in the Wild "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "sys.path.insert(0, '../../Utils')\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import *  \n",
    "from data_downloaders import *\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labeled Faces in the Wild \n",
    "### http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lfw('../../Datasets/')\n",
    "\n",
    "\n",
    "data_dir = \"../../Datasets/lfw/lfw_20/\"\n",
    "\n",
    "img_paths = []\n",
    "for p in os.listdir(data_dir): \n",
    "    for i in os.listdir(os.path.join(data_dir,p)): \n",
    "        img_paths.append(os.path.join(data_dir,p,i))\n",
    "        \n",
    "people = []\n",
    "people_to_idx = {}\n",
    "k = 0 \n",
    "for i in img_paths: \n",
    "    name = i.split('/')[-2]\n",
    "    if name not in people_to_idx: \n",
    "        people.append(name)\n",
    "        people_to_idx[name] = k\n",
    "        k += 1\n",
    "\n",
    "\n",
    "n_classes = len(people)\n",
    "\n",
    "img_paths = np.random.permutation(img_paths)\n",
    "\n",
    "lfw_size = len(img_paths)\n",
    "\n",
    "lfw_train_size = int(0.8 * lfw_size)\n",
    "\n",
    "lfw_train_list = img_paths[:lfw_train_size]\n",
    "lfw_test_list = img_paths[lfw_train_size:]\n",
    "\n",
    "class LFWDataset(Dataset): \n",
    "    def __init__(self, file_list, class_to_label, transform=None): \n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.people_to_idx = class_to_label\n",
    "        \n",
    "                \n",
    "    def __len__(self): \n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = self.file_list[idx]\n",
    "        image = io.imread(img_path)\n",
    "        label = self.people_to_idx[img_path.split('/')[-2]]\n",
    "        \n",
    "        if self.transform is not None: \n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "\n",
    "# Data augmentation \n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.RandomRotation(10),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    #torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    " \n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "    \n",
    "\n",
    "trainset = LFWDataset(lfw_train_list, people_to_idx, transform=train_transform)\n",
    "testset = LFWDataset(lfw_test_list, people_to_idx, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# helper function to unnormalize and plot image \n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    img = img / 2 + 0.5\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "# display sample from dataset \n",
    "imgs,labels = iter(trainloader).next()\n",
    "imshow(torchvision.utils.make_grid(imgs))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = models.tiny_cnn(n_in=3, n_out=n_classes, n_hidden=32, size=250).to(device)\n",
    "\n",
    "conv_net.apply(models.weights_init)\n",
    "\n",
    "conv_optim = optim.Adam(conv_net.parameters(), lr=lr)\n",
    "\n",
    "train(conv_net, trainloader, testloader, conv_optim, loss, n_epochs, verbose=False)\n",
    "\n",
    "print(\"\\nPerformance on training set: \")\n",
    "train_accuracy = eval_target_net(conv_net, trainloader, classes=None)\n",
    "\n",
    "print(\"\\nPerformance on test set: \")\n",
    "test_accuracy = eval_target_net(conv_net, testloader, classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Train Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the torchvision resnet18 implementation \n",
    "resnet18 = torchvision.models.resnet18(num_classes=n_classes).to(device)\n",
    "\n",
    "resnet18.fc = nn.Linear(2048, n_classes)\n",
    "\n",
    "resnet18.apply(models.weights_init)\n",
    "\n",
    "resnet18_optim = optim.Adam(resnet18.parameters(), lr=lr)\n",
    "\n",
    "resnet18 = resnet18.to(device)\n",
    "train(resnet18, trainloader, testloader, resnet18_optim, loss, n_epochs, verbose=False)\n",
    "\n",
    "print(\"\\nPerformance on training set: \")\n",
    "train_accuracy = eval_target_net(resnet18, trainloader, classes=None)\n",
    "\n",
    "print(\"\\nPerformance on test set: \")\n",
    "test_accuracy = eval_target_net(resnet18, testloader, classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(num_classes=n_classes)\n",
    "\n",
    "vgg16.apply(models.weights_init)\n",
    "\n",
    "\n",
    "vgg16_optim = optim.SGD(vgg16.parameters(), lr=lr)\n",
    "#vgg16_optim = optim.Adam(vgg16.parameters(), lr=lr)\n",
    "\n",
    "vgg16 = vgg16.to(device)\n",
    "train(vgg16, trainloader, testloader, vgg16_optim, loss, n_epochs, verbose=False)\n",
    "\n",
    "print(\"\\nPerformance on training set: \")\n",
    "train_accuracy = eval_target_net(vgg16, trainloader, classes=None)\n",
    "\n",
    "print(\"\\nPerformance on test set: \")\n",
    "test_accuracy = eval_target_net(vgg16, testloader, classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.AlexNet(n_classes=n_classes, size=250).to(device)\n",
    "\n",
    "alexnet.apply(models.weights_init)\n",
    "\n",
    "alexnet_optim = optim.Adam(alexnet.parameters(), lr=lr/10)\n",
    "\n",
    "train(alexnet, trainloader, testloader, alexnet_optim, loss, n_epochs, verbose=False)\n",
    "\n",
    "print(\"\\nPerformance on training set: \")\n",
    "train_accuracy = eval_target_net(alexnet, trainloader, classes=None)\n",
    "\n",
    "print(\"\\nPerformance on test set: \")\n",
    "test_accuracy = eval_target_net(alexnet, testloader, classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n",
    "Performance on training set: Accuracy = 99.79 %\n",
    "\n",
    "Performance on test set: Accuracy = 49.59 %\n",
    "\n",
    "\n",
    "Renset18\n",
    "Performance on training set: Accuracy = 97.93 %\n",
    "\n",
    "Performance on test set: Accuracy = 12.07 %\n",
    "\n",
    "\n",
    "VGG16\n",
    "Performance on training set: Accuracy = 100.00 %\n",
    "\n",
    "Performance on test set: Accuracy = 58.84 %\n",
    "\n",
    "\n",
    "AlexNet\n",
    "Performance on training set: Accuracy = 99.46 %\n",
    "\n",
    "Performance on test set: Accuracy = 65.62 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
